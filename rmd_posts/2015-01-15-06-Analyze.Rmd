---
title: "06 - Analyze: Packages"
author: Jeffrey W. Hollister
layout: post_page
---

```{r setup, echo=FALSE, warning=FALSE, purl=FALSE, message=FALSE}
options(repos="http://cran.rstudio.com/")
if(!require("ggplot2")){
  install.packages("ggplot2")
}
if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("randomForest")){
  install.packages("randomForest")
}
if(!require("XML")){
  install.packages("XML")
}
if(!require("RCurl")){
  install.packages("RCurl")
}

library("ggplot2")
library("dplyr")
library("XML")
library("RCurl")
```

The second section will run through a fairly quick example of using a package, `randomForest` for some analysis.  I've included this section just to show how (relatively) trivial it is to add an advanced capability to R via packages.

##Quick Links to Exercises and R code
- [Lesson 6 R Code](/introR/rmd_posts/2015-01-15-06-Analyze.R): All the code from this post in an R Script.
- [Exercise 1](#exercise-1): Use random forests to predict NLA reference class. 

##Lesson Goals
- Understand how to add statistical functionality to R via packages
- See some of the best ways to learn a new package

Random forests were developed by Leo Breiman at UC-Berkely ([Breiman 2001](http://dx.doi.org/10.1023/A:1010933404324)).  At their simplest, a random forest is a collection (or ensemble) of classification (or regression) trees.  Each tree is developed based on a resampling of both the data and variables.  They have been shown to have superior predictive perfomance as well as to be robust to violation of the standard suite of assumptions that often plague other approaches.  In my opinion, if you want to learn one machine learning approach, this is it.  If you want to read more, a few good references are:

- [Cutler et. al. (2007):Random forests for classification in ecology](http://dx.doi.org/10.1890/07-0539.1)
- [FernÂ´andez-Delgado et al. 2014: Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf) (HT:[Simply Statistics](http://simplystatistics.org/2014/12/17/a-non-comprehensive-list-of-awesome-things-other-people-did-in-2014/))


The implementation in R is accessed through the `randomForest` package.  Let's install this, load it up and start to look around.

```{r rf_actual_install, echo=FALSE, include=FALSE, purl=FALSE}
if(!require("randomForest")){
  install.packages("randomForest")
}
library("randomForest")
```

```{r rf_install, eval=FALSE}
install.packages("randomForest")
library("randomForest")
help(package="randomForest")
```

There are quite a few functions included with this package, but the one we are most interested in is the workhorse `randomForest()` function.  For our examples we will look at classification trees.  The bare minimum we need for this to work are just our independent (x) and dependent variables (y).  Let's see if we can predict iris species from the morphology measurements...

```{r rf_example}
rf_x<-select(iris,Petal.Width, Petal.Length, Sepal.Width, Sepal.Length)
rf_y<-select(iris,Species)
rf_y<-rf_y$Species
iris_rf<-randomForest(x=rf_x,y=rf_y)
iris_rf
```

Can we predict species, well, I'd say so!  

We can also call `randomForest()` using a formula for a data frame like,

```{r rf_form_examp}
iris_rf2<-randomForest(Species~.,data=iris)
iris_rf2
```

The only thing we haven't seen before here is the ".".  That indicates all the other variables in the data frame.  It is just a shortcut and you could explicitly name the variables to use.

Lastly, we can look at a couple of plots that tell us a little bit more about the model with some of the plotting functions that come with randomForest.

```{r rf_plots}
#Error vs num of trees
plot(iris_rf2)
#Variable Importance
varImpPlot(iris_rf2)
```

The default plot shows error for the total model and and each of the classes as a function of the number of trees. Variable importance is plotting the decrease in Gini as each variable is added to the model.  

For the basics, that is it!  Let's try it now on the NLA data.

```{r Exercise1, echo=FALSE}
```

##Exercise 1
For this exercise we are going to implement random forest and try to predict the NLA reference class from the water quality measurements.

1. Add a new section to the script
2. Build your model to predict RT_NLA.  You may specify the model however makes the most sense
3. Print to the screen your result, plot the error and variable importance. When finished put a green sticky up and we will come around to check.

##Learning a New Package

As of `r format(Sys.Date(), "%b %d, %Y")`, there were `r nrow(readHTMLTable("http://cran.r-project.org/web/packages/available_packages_by_date.html")[[1]])` packages available on [CRAN](http://cran.r-project.org/web/packages/).  Given this diversity and since these packages are created and maintained by many different authors, the ways in which you can get help on a specific package and the quality of that assistance can vary greatly.  That being said, there are a few indicators of decent help for a given package.

First, if a package has a vignette that is usually a good first place to start.  To list the vignettes for a given package you can use the `vignette()` function.  For instance:

```{r vignette_examp,eval=FALSE}
vignette(package="knitr")
```

Second, it is becoming increasingly common to see journal articles about packages.  Many journals now accept software manuscripts, but the journals I most often use for finding out about new R packages are:

*note: This is a VERY incomplete list...*

- [Journal of Statistical Software](http://www.jstatsoft.org/)
- [R Journal](http://journal.r-project.org/)
- [F1000 Research](http://f1000research.com/search?q=R%20Package&sortingBy=&sortingOrder=&indexed=&articleTypes=SOFTWARE_TOOLS)
- [PLoS One](http://www.plosone.org/search/simple?from=globalSimpleSearch&filterJournals=PLoSONE&query=R+Package&x=0&y=0)

Third, the last resort is of course [Google](http://www.google.com)!  



