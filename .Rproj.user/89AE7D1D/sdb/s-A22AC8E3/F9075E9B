{
    "contents" : "---\ntitle: \"03 - Clean\"\nauthor: Jeffrey W. Hollister\nlayout: post_page\n---\n\nIn my experience, most data analysis and statistics classes seem to assume that 95% of the time spent working with data is on the analysis and interpretation of that analysis and little time is spent getting data ready to analyze and, therefore, very little time is devoted to learning how to do this. However, in reality, I'd argue, the time spent is flipped with most time spent on cleaning up data and significantly less time on the analysis.    \n\nCleaning data (or data munging, data jujitsu, data wrangling) often happens at two points.  The first stage is getting raw data files ready to be read in by the computer.  This can be quite the challenge.  For the purposes of this workshop we are not going to focus on that side too much.  The data files we are using are in a clean enough format to be read in directly.  If you want to learn more about some best practices in maintaining good clean data files, I'd point you to two places.  First is some materials from [Data Carpentry on working with spreadsheets](https://github.com/datacarpentry/datacarpentry/blob/master/lessons/excel/ecology_spreadsheets.md) and second is [Hadley Wickham's Tidy Data paper](http://www.jstatsoft.org/v59/i10/paper).\n\nFrom here on out we are assuming that we have a raw data file that is ready to be read into R and that we have already done that (we did this in [Lesson 2:Exercise 2](/introR/2015/01/14/02-Get/#exercise-2)).  While the data files are in pretty good shape at this point, there are often tasks that we need to do to select out portions of the data, rearranage the data, add new columns, merge multiple files, etc.  These are the tasks that we will focus on over the next two lessons.  This lesson will deal with using tools we have available to us in Base R.  Next lesson will focus on a new package `dplyr` that should make some of this work a bit easier.  \n\n##Subset\nBut first, lets start exploring indexing to subset both observations (the rows) and variables (the columns).\n\nSome of the commands we know already, like `head()` and `tail()` kind of do this, but we need more control than that.  We get that with indexing.  In short indexing allows you to specify individual (or ranges) of rows and columns.  We can index data frames and vectors (and lists and matrices, but we aren't going to go into that).  Let's start with a vector example and build from there.\n\n```{r indexing_examp}\n#Create a vector\nx<-c(10:19)\nx\n#Postive indexing returns just the value in the ith place\nx[7]\n#Negative indexing returns all values except the value in the ith place\nx[-3]\n#Ranges work too\nx[8:10]\n#A vector can be used to index\n#Can be numeric\nx[c(2,6,10)]\n#Can be boolean - will repeat the pattern \nx[c(TRUE,FALSE)]\n#Can even get fancy\nx[x%%2==0]\n```\n \nSo that's kinda handy.  In practice you wouldn't likely just be asking for that information to be returned to the screen, you'd probably save the output to a new object, kind of like `evens<-x[x%%2==0]`.  Let's move on now and look at indexing data frames.\n\nSince a data frame has two dimensions, you need to specify an index for both the row and the column.  You can specify both and get a single value like `data_frame[row,column`],specify just the row and the get the whole row back like `data_frame[row,]` or get just the column with `data_frame[,column]`.  Let's work some examples.\n\n```{r data_frame_index}\n#Let's use one of the stock data frames in R, iris\nhead(iris)\n#And grab a specific value\niris[1,1]\n#A whole column\npetal_len<-iris[,3]\npetal_len\n#A row\nobs15<-iris[15,]\nobs15\n#Many rows\nobs3to7<-iris[3:7,]\nobs3to7\n```\n\nSo that's the basics.  Still in the arena of kinda useful, but not really... Remember though that we can index vectors with other vectors and a data frame is just a collection of vectors (kinda) and that starts to get interesting.  Also remember that data frames have column names.  We can use those too.  Let's try it.\n\n```{r more_data_frame_index}\n#First, there are a couple of ways to use the column names\niris$Petal.Length\nhead(iris[\"Petal.Length\"])\n#Multiple colums\nhead(iris[c(\"Petal.Length\",\"Species\")])\n#Now we can combine what we have seen to do some more complex queries\n#Lets get all the data for Species with a petal length greater than 6\nbig_iris<-iris[iris$Petal.Length>=6,]\nhead(big_iris)\n#Or maybe we want just the sepal widths of the virginica species\nvirginica_iris<-iris$Sepal.Width[iris$Species==\"virginica\"]\nhead(virginica_iris)\n```\n\nGot it?!  Probably sort of!?  I will admit that the syntax for subsetting data this way is a bit obtuse.  I did want to make sure you all saw it though becuase it comes up often in other peoples code and helps you get a better understanding, I think, of what you are doing.  That being said there are other options.  In base, `subset()` and also in `dplyr` which will be the next lesson.\n\n```{r subset_examp}\n#And redo what we did above\nbig_iris_subset<-subset(iris,subset=Petal.Length>=6)\nhead(big_iris_subset)\nvirginica_iris_subset<-subset(iris,subset=Species==\"virginica\",select=Sepal.Width)\nhead(virginica_iris_subset)\n```\n\nSo, I think that is a big improvement in the syntax over the raw indexing and certainly lends itself to more understandable code.\n\n##Exercise 1\nThis exercise is going to focus on using what we just covered on indexing and subsetting to start to clean up the National Lakes Assessment data files.  Remember to use the stickies: green when your done, red if you have a problem.\n\n1. If it isn't already open, make sure you have the script we created, \"nla_analysis.R\" opened up.\n2. Start a new section of code in this script by simply putting in a line or two of comments indicating what it is this set of code does.\n3. Our goal for this is to create two new data frames that represent a subset of the observations as well as a subset of the data. \n4. First, from the  `nla_sites` data frame we want a new data frame that has only the following columns: SITE_ID, LON_DD, LAT_DD, STATE_NAME, WSA_ECO9, NUT_REG, NUTREG_NAME, LAKE_ORIGIN, and RT_NLA.  Name the new data frame `nla_sites_subset`.\n5. Next, lets subset the water quality data from `nla_wq`.  The columns we want for this are: SITE_ID, VISIT_NO, SITE_TYPE, TURB, NTL, PTL, CHLA, and SECMEAN. Call this `nla_wq_subset`.\n6. Last thing we are going to need to do is get a subset of the observations from `nla_wq_subset`.  We need only the lakes that with VISIT_NO equal to 1 and SITE_TYPE equal to \"REF_Lake\".  Keep the same name, `nla_wq_subset`, for this data frame.\n\n##Merging Data\nAnother very common clean-up/data managment task is to be able to merge data into a single data frame.  We are going to talk about several different ways to do this.  First, is adding a new column or existing vector to a data frame with `cbind()`,`data.frame()`,or`$`. \n\n```{r cbind_examp}\n#Create a new vector to add to the iris data frame\ncategories<-sample(1:3,nrow(iris),replace=T)\niris_cbind<-cbind(iris,categories)\nhead(iris_cbind)\n#Can also use data.frame again for this\niris_cbind_df<-data.frame\nhead(iris_cbind_df)\n#Direct assignment\niris_cbind_dollar<-iris\niris_cbind_dollar$categories<-sample(1:3,nrow(iris),replace=T)\nhead(iris_cbind_dollar)\n```\n\nThese are all handy to know, but you need to be very careful with it.  First, the data in the vector you are adding needs to be in the same order as the observations in the data frame.  Also it needs to have the same number of rows (or divisible by that number of rows).  \n\nThe next thing we might want to do is add some new rows to a data frame.  This is very handy as you might have data collected and entered at one time, and then additional observations made later that need to be added.  So with `rbind()` we can stack two data frames with the same columns to store more observations.  \n\n```{r rbind_examp}\n#Let's first create a new small example data.frame\nrbind_df<-data.frame(a=1:3,b=c(\"a\",\"b\",\"c\"),c=c(T,T,F),d=rnorm(3))\n#Now an example df to add\nrbind_df2<-data.frame(a=10:12,b=c(\"x\",\"y\",\"z\"),c=c(F,F,F),d=rnorm(3))\nrbind_df<-rbind(rbind_df, rbind_df2)\nrbind_df\n```\n\nNow something to think about.  Could you add a vector as a new row?  Why/Why not? When/When not?\n\nLet's go back to the columns now.  What we can currently do is add columns that need to be same length, or length that is divisible by number of rows in the original, and need to be in the same order.  But it is very common to have to datasets that are in different orders and have differing numbers of rows.  What we want to do in that case is going to be more of a database type function and join two tables based on a common column.  A common way to do that in base R is with `merge()`.   So let's contrive another example by creating a dataset to merge to `rbind_df` that we created above.\n\n```{r merge_example}\n# Contrived data frame\nrbind_df_merge_me<-data.frame(a=c(1,3,10,11,14,6,23),x=rnorm(7),names=c(\"bob\",\"joe\",\"sue\",NA,NA,\"jeff\",NA))\n# Create merge of matches\nrbind_df_merge_match<-merge(rbind_df,rbind_df_merge_me,by=\"a\")\nrbind_df_merge_match\n# Create merge of matches and all of the first data frame\nrbind_df_merge_allx<-merge(rbind_df,rbind_df_merge_me,by=\"a\",all.x=TRUE)\nrbind_df_merge_allx\n```\n\n##Exercise 2\nIn this exercise we are going to practice merging our NLA data.  You will remember that we have two datasets, one on water quality and one on site information.  We selected some info out of the water quality data that we didn't select out of the site info so we have two data frames, with differing numbers of rows and unknown order.  Use your stickies!\n\n1. This is the only task we have for this exercise.  Add to your script a line (or more if you need it) to create a new data frame, `nla_data`, that is a merge of `nla_site_subset` and `nla_wq_subset`, but with all lines in `nla_wq_subset` preserved in the output.\n2. If that goes quickly, feel free to explore `rbind()` some.\n\n##Reshape and Modifying\nSo we now have a data frame that is set up pretty well to facilitate some future analysis.  That being said, different analyses and visualizations are probably going to require some additional monekying around with our data.  This class of \n\n##Exercise 3\n",
    "created" : 1419013571619.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1864801901",
    "id" : "F9075E9B",
    "lastKnownWriteTime" : 1419013587,
    "path" : "~/projects/introR/rmd_posts/2015-01-14-03-Clean.Rmd",
    "project_path" : "rmd_posts/2015-01-14-03-Clean.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}